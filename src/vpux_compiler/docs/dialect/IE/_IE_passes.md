<!-- Autogenerated by mlir-tblgen; don't manually edit -->
### `-adapt-shapes-for-scale-shift`: Adjusts 2-d and 3-d `IE.Add` and `IE.Multiply` for further conversion to `IE.ScaleShift`
Converts these subgraphs:
```
    Input tensor<NxM> => IE.Add : tensor<NxM>, tensor<1xM> -> tensor<NxM>
    Input tensor<NxM> => IE.Multiply : tensor<NxM>, tensor<1xM> -> tensor<NxM>
    Input tensor<1xNxM> => IE.Add : tensor<1xNxM>, tensor<1x1xM> -> tensor<1xNxM>
    Input tensor<1xNxM> => IE.Multiply : tensor<1xNxM>, tensor<1x1xM> -> tensor<1xNxM>
```
Into the following subgraphs respectively:
```
    Input tensor<NxM> => IE.Add : tensor<1xMxNx1>, tensor<1xMx1x1> => tensor<NxM>
    Input tensor<NxM> => IE.Multiply : tensor<1xMxNx1>, tensor<1xMx1x1> => tensor<NxM>
    Input tensor<1xNxM> => IE.Add : tensor<1xMxNx1>, tensor<1xMx1x1> => tensor<1xNxM>
    Input tensor<1xNxM> => IE.Multiply : tensor<1xMxNx1>, tensor<1xMx1x1> => tensor<1xNxM>
```
The following shape transformations will be applied for 2-d case:
```
    Input NxM => Reshape 1xNxMx1 => Transpose 1xMxNx1 => Add => Transpose 1xNxMx1 => Reshape NxM
```
For 3-d case:
```
    Input 1xNxM => Reshape 1xNxMx1 => Transpose 1xMxNx1 => Add => Transpose 1xNxMx1 => Reshape 1xNxM
```
It is also possible to apply reshape to get `IE.Add : tensor<NxMx1x1>, tensor<1xMx1x1>`.
However, such approach may lead to a big cluster of NCE tasks after `UnrollBatch` pass.
The measurements show that transposition is more effective for this pass.
### `-adjust-convolution-input-shape`: Adjust convolution input shape for better hardware utilization
This pass adjusts convolution input shape for better hardware utilization

Original operation:
    Activation: 1x32x256x1 ->
                            Conv -> 1x16x256x1
    Weights:    16x32x1x1 ->

New subgraph:
    Activation: 1x32x256x1     Weights:16x32x1x1
        |                       |
    Reshape: 1x32x64x4          |
        |                       |
        |                       |
        |                       |
             Conv: 1x16x64x4
                    |
             Reshape: 1x16x256x1
                    |
### `-adjust-groupconv-shape`: Adjust Goupconvolution input shape and kernel shape to avoid or reduce expand size for channel align request
This pass adjusts Groupconvolution input shape and kernel shape to get a better performance

For the shape can reserve a channel aligned shape:
Original operation:
    Activation: 1x2x64x512 ->
                            Conv -> 1x2x64x512
    Weights:    2x1x1x1 ->

New subgraph:
    Activation: 1x2x64x512     Weights:16x1x1x1
        |                       |
    Reshape: 1x16x8x512         |
        |                       |
        |                       |
        |                       |
             Conv: 1x16x8x512
                    |
             Reshape: 1x2x64x512
                    |

For the shape can's reserve a channel aligned shape:
    Original operation:
        Activation: 1x1x289x289 ->
                                Conv -> 1x1x289x289
        Weights:    1x1x1x1     ->

    New subgraph:
        Activation: 1x1x289x289     Weights:289x1x1x1
            |                       |
        ShapeCast: 1x289x17x17      |
            |                       |
            |                       |
             \                     /
                Conv: 1x289x17x17
                        |
            ShapeCast: 1x1x289x289
                        |
### `-adjust-input-shape-for-eltwise`: Reshape the activation inputs of eltwise ops to make the inputs' channel aligned without `IE.Expand`
Insert ShapeCast Ops to the inputs of eltwise ops when the input channels require alignment.
Insert ShapeCast Ops to the outputs before the next non-eltwise op.
### `-adjust-layouts`: Adjust required layouts for all layers
The pass is a part of `IECommon` pipeline.

This pass adds the required layouts instead of the default one
depending on the layer specification from underlying Dialect.

#### Options
```
-se-ops-enabled : Flag to identify whether operations that can be executed using the Storage Element hardware feature are enabled
```
### `-adjust-mem-permute-around-op`: Adjust MemPermuteOps around to reduce number of real permutes
Adjust MemPermuteOps around an operation to avoid unnecessary permutes
For example, a possible conversion from
```
    IE.LayerOp -> IE.MemPermute \
                                 IE.Eltwise -> IE.MemPermute -> IE.LayerOp
                     IE.LayerOp /
```
to
```
                      IE.LayerOp \
                                  IE.Eltwise -> IE.LayerOp
    IE.LayerOp -> IE.PermuteCast /
```
### `-adjust-scale-shift-for-dw-conv`: Adjust ScaleShift for DW Convolution
Adjust input N > 1 scaleShift to N = 1 by broadcast and reshape,
for preventing the generation of the large number of DW convolution fragments.

If the activation is Const, will adjust ScaleShift regardless of the size of N.

If the activation is not Const, will adjust ScaleShift only when N > 16 (rough
estimates obtained from experiments).
### `-adjust-software-ops-precision`: Adjust precision of software ops to satisfy kernel implementation
The pass is a part of `AdjustPrecision` pipeline.

Some kernel implementations only support specific precisions. To satisfy this requirement,
such ops are surrounded by conversion layers.
### `-align-scales`: Align FQ ranges around Concat layers.
This pass aligns FQ ranges around Concat ops and inserts Clamp ops to keep the valuse in the original ranges.

Original subgraph:
Conv1                 ARG
  |                    |
FQ1(range1)     FQ2(range2)
              |
            Concat
              |
            Conv2

New subgraph:
Conv1                                               ARG
  |                                                  |
FQ(newRange)                                        FQ(newRange)
  |                                                  |
Clamp(low and hight from original FQ)               Clamp(low and hight from original FQ)
                                            |
                                          Concat
                                            |
                                          Conv2
### `-broadcast-input-for-add`: Broadcast input for Add op
This pass broadcast input for AddOp when the input1's shape isn't equal to input2's shape which cannot convert to ScaleShift.
### `-convert-assign-read-value`: Convert assign to returns and read value to inputs
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces `Assign` operations with main function returns and
`ReadValue` operations with main function inputs.
### `-convert-avg-pool-to-dw-conv`: Convert AvgPool op to GroupConvolution op
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces suitable `AvgPool` operations with `GroupConvolution` operation.
### `-convert-batched-conv-to-1n`: Convert Convolution with batched input to new one with batch equal to 1
This pass inserts Transpose to convert batched input to new one with batch equal to 1

Original operation:
    Activation: 4x16x1x1 ->
                            Conv -> 4x5x1x1
    Weights:    5x16x1x1 ->

New subgraph:
    Activation:4x16x1x1     Weights:5x16x1x1
        |                       |
    Transpose:1x16x4x1          |
        |                       |
        |                       |
                Conv:1x5x4x1
                    |
             Transpose:4x5x1x1
### `-convert-bilinear-to-strided-concat-and-conv`: Convert bilinear interpolate op to strided concat, MaxPool and some depthwise convolution Ops
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces `Bilinear Interpolate` operations with `Concat` operations with strides,
MaxPool and some `depthwise` convolutions.

In case the `interpolateAsSEOp` option is set to true, only cases that cannot be executed
using the Storage Element hardware feature will be converted to concats & NCE ops.

#### Options
```
-interpolate-as-se-op : Flag which identifies whether an Interpolate operation can be executed using the Storage Element hardware feature
```
### `-convert-broadcast-to-tile`: Convert Broadcast operation to Tile operation
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces `Broadcast` operation with `Tile` operation.
### `-convert-deconv-to-conv`: Convert Deconvolution 2D to Convolution 2D
The pass is a part of `AdjustForVPU` pipeline.

Replaces deconvolution by upsampling and convolution
### `-convert-depthToSpace`: Convert DepthToSpace layer to {reshape -> transpose -> reshape} subgraph
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `DepthToSpace` operations with {reshape -> transpose -> reshape} subgraph.
### `-convert-expand-to-conv`: Convert IE.Expand to IE.Reshape -> IE.Convolution -> IE.Reshape
Replace NHWC IE.Expand with IE.Convolution.
1. Reshape input from [1, IC, H, W] to [1, IC * 16, H, W / 16]
For example 1x3x480x640 becomes 1x48x480x40
2. Compose IE.Convolution that has OC = (IC + padIC) * 16.
Padded channels must be multiplied by zero.
Activation channels must be multiplied by 1.
3. Reshape output back to original shape [1, IC + padIC, H, W]
For example when padIC = 13, OC = (3 + 13) * 16 = 256
When padIC = 1, OC = (3 + 1) * 16 = 64
Weights structure goes like this for 3 input channels and 4 output channels:
| idx   |    0 |    1 |    2 |    3 |    4 |    5 |    6 |    7 |  ... |   46 |   47 |
| ----- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| OC 0  |    1 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 1  |    0 |    1 |    0 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 2  |    0 |    0 |    1 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 3  |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 4  |    0 |    0 |    0 |    1 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 5  |    0 |    0 |    0 |    0 |    1 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 6  |    0 |    0 |    0 |    0 |    0 |    1 |    0 |    0 |  ... |    0 |    0 |
| OC 7  |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
| OC 8  |    0 |    0 |    0 |    0 |    0 |    0 |    1 |    0 |  ... |    0 |    0 |
| OC 9  |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    1 |  ... |    0 |    0 |
| ...   |  ... |  ... |  ... |  ... |  ... |  ... |  ... |  ... |  ... |  ... |  ... |
| OC 62 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    1 |
| OC 63 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |    0 |  ... |    0 |    0 |
### `-convert-extract-image-patches`: Converts subgraphs around ExtractImagePatches into some more optimal for VPU ones when the necessary conditions are met
Converts these subgraphs:
```
    IE.ReduceSum -> IE.ExtractImagePatches -> IE.Transpose -> IE.ReduceSum
    IE.ReduceSum -> IE.ExtractImagePatches -> IE.ReduceSum
    IE.ExtractImagePatches -> IE.Transpose -> IE.AffineReshape
    IE.ExtractImagePatches -> IE.Transpose
    IE.ExtractImagePatches
```
Into the following subgraphs respectively:
```
    IE.ReduceSum -> IE.Unsqueeze
    IE.ReduceSum -> IE.Unsqueeze
    N x IE.Slice -> IE.Concat
    N x IE.Slice -> IE.Concat -> IE.AffineReshape
    IE.Transpose
```
### `-convert-fc-to-conv`: Convert FullyConnected op to Convolution operation
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `FullyConnected` operations with `Convolution` operation.
It inserts extra `Reshape` operations to satisfy `Convolution` specification.
### `-convert-gather-to-slice`: Convert Gather operation to Slice operation
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces legal `Gather` operations with `Slice` operations.
### `-convert-grn-to-normalizel2`: Convert GRN operation to Normalize_L2 operation
This pass replaces `GRN` with `Normalize_L2` operation. `GRN` is a `Normalize_L2` operation with specified axes input and processing mode whose values are fixed in all possible cases, also there is no dedicated kernel for  `GRN` operation.
### `-convert-groupconv-to-conv`: Convert GroupConvolution to Convolution
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces some `GroupConvolution` operations with `Convolution` operation.
It inserts extra `Slice` and `Concat` operations to satisfy `Convolution` specification.
### `-convert-groupdeconv-to-groupconv`: Convert GroupDeconvolution to GroupConvolution
The pass replaces IE::GroupDeconvolution by IE::Upsampling and IE::GroupConvolution
### `-convert-matmul-to-conv`: Convert MatMul with 2d 'weights' to convolution
This pass replaces 2d `Matmul` operations with `Convolution` .
### `-convert-mem-permute-to-pool`: Converts an `IE.MemPermute` operation to an `IE.MaxPool` operation
Replace an `IE.MemPermute` operation with a chain of
```
    IE.ShapeCast (reinterpret mem_perm) -> IE.LayoutCast (to NHWC) -> IE.MaxPool (ODU permute) -> IE.LayoutCast (to dst_order) -> IE.ShapeCast
```
Input IE.ShapeCast swaps the dimensions of the initial IE.MemPermute.
Input IE.LayoutCast overrides the layout of an input with NHWC (required by NCE tasks).
Output IE.LayoutCast is needed since dst_order may differ from the ODU permute order.
Output IE.ShapeCast restores the shape of the original IE.MemPermute output.

Only partial support is available:
* IE.MemPermute (NCHW input, NCHW dst_order, NHCW mem_perm)
* IE.MemPermute (NCHW input, NCHW dst_order, NHWC mem_perm)
* IE.MemPermute (NHCW input, NCHW dst_order, NHCW mem_perm)
* IE.MemPermute (NCWH input, NHWC dst_order, NWHC mem_perm)
### `-convert-nce-ops-to-4d`: Convert non 4D NCE tasks to its 4D variance
The pass is a part of `AdjustForVPU` pipeline.

Extends input, filter and output tensors with height = 1.
[N, C, W] -> [N, C, 1, W]
strides:    {2} -> strides:    {1, 2}
pads_begin: {2} -> pads_begin: {0, 2}
pads_end:   {2} -> pads_end:   {0, 2}
dilations:  {2} -> dilations:  {1, 2}
### `-convert-nearest-to-broadcast-or-strided-concat`: Convert nearest interpolate op to broadcast or strided concat ops
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces `Nearest Interpolate` operations with `Broadcast` or `Concat` operations with strides.

In case the `interpolateAsSEOp` option is set to true, only cases that cannot be executed
using the Storage Element hardware feature will be converted to `Broadcast` or `Concat`.

#### Options
```
-interpolate-as-se-op : Flag which identifies whether an Interpolate operation can be executed using the Storage Element hardware feature
```
### `-convert-pad-to-concat`: Convert Pad Ops to Concat with Constant
The pass is a part of `AdjustForVPU` pipeline.

After FusePadOps pass, there are Pad Ops can not be fused.
Replace `IE::PadOp` with `IE::ConcatOp` and `Const::DeclareOp`
Only `IE::PadMode::CONSTANT` case is supported.
### `-convert-paddings-to-floor-mode`: Convert Convolution and Pooling layers paddings to FLOOR rouding mode
The pass is a part of `AdjustForVPU` pipeline.

This pass updates padding attributes for Convolution and Pooling layers.
It switches layer rounding mode to FLOOR and updates paddings to satisfy output shape.
### `-convert-power-to-mult`: Convert power to multiply operation
The pass converts power with single constant exponent value to multiplication.
### `-convert-precision-to-fp16`: Convert tensors precision from FP32 to FP16
The pass is a part of `AdjustPrecision` pipeline.

This pass replaces all FP32 tensors with FP16.
It updates both function bodies as well as Function signatures.
### `-convert-precision-to-i32`: Convert tensors precision from I64 to I32
The pass is a part of `AdjustPrecision` pipeline.
This pass replaces all I64 tensors with I32.
It updates both function bodies as well as Function signatures.
### `-convert-quantize-ops-to-nce-ops`: Converts per-tensor Quantize/Dequantize to eltwise And mixed-precision operation
The pass is a part of `LowPrecision` pipeline.

Converts per-tensor Quantize/Dequantize to eltwise And mixed-precision operation
where input2 is input1 to perform type conversion on DPU instead of UPA.
### `-convert-reduce-sum-to-conv`: Convert ReduceSum to Convolution operation
The pass is to convert ReduceSum operation into Convolution.
### `-convert-reduce-to-pooling`: Convert reduce to pooling ops
The pass is to convert reduce operations (mean, max, sum, min) into pooling.
### `-convert-reflect-pad-to-slice-and-concat`: Convert reflect pad to slice and concat
Convert reflect pad to slice and concat
### `-convert-reorder-to-permute-quantize`: Converts IE.Reorder to DPU permute
Converts IE.Reorder with float16 input and float16 output to DPU permute.
### `-convert-scalar-to-tensor`: Convert a scalar input to tensor
This pass checks the operands/results rank for any operation and if it is a scalar(its rank is 0), it will be converted into a tensor with one element.
### `-convert-scale-shift-depthwise`: Convert Scale-Shift operation to Depthwise Convolution
The pass is a part of `HardwareMode` pipeline.

Convert Scale-Shift operation to Depthwise convolution.
### `-convert-scatterndupdate-to-strided-concat`: Convert ScatterNDUpdate op to strided concat ops
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces `ScatterNDUpdate` operations with `Concat` operations with strides.
### `-convert-shape-to-4d`: Convert tensors shapes to 4D
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces ND tensor with 4D analogues for layers, which has such limitations on VPUIP level.
Also this pass replaces ND network inputs and outputs with 4D analogues to overcome runtime limitations.
### `-convert-shuffle-channels`: Convert ShuffleChannels to Reshape->Transpose->Reshape
The pass is a part of `AdjustForVPU` pipeline.
Converts ShuffleChannels to Reshape->Transpose->Reshape.
### `-convert-spaceToDepth`: Convert SpaceToDepth layer to {reshape -> transpose -> reshape} pattern
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `SpaceToDepth` operations with {reshape -> transpose -> reshape} pattern.
### `-convert-squareddiff-to-subpower`: Convert SquaredDifference operation to Subtract and Power
This pass converts SquaredDifference operation to its equivalent (x-y)^2
### `-convert-strided-slice-to-dwconv`: Convert strided slice to dwconv
The pass is a part of `AdjustForVPU` pipeline.
The pass replaces `StridedSlice` with strides > 1 operations with `GroupConvolution` operation.
### `-convert-subtract-to-add`: Convert Subtract operation to Add with either Negative or DW Conv operations
This pass replaces `Subtract` operation with `Add` with `Negative` operations on VPUX30XX or `Add` with `DW Conv` operations on VPUX37XX.
### `-convert-to-mem-permute`: Convert Reorder and Transpose ops to MemPermute operation
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `Reorder` and `Transpose` operations with `MemPermute` operation.
### `-convert-to-mixed-precision`: Convert DPU task without fake quantize behind to mixed-precision operation
The pass is a part of `LowPrecision` pipeline.
Converts DPU task to mixed-precision operation where there is no quantize operation for the output of a DPU task
### `-convert-to-scale-shift`: Convert Add and Multiply operations to ScaleShift operations
This pass replaces suitable `Add` and `Multiply` operations with `ScaleShift` operations.
### `-convert-upsampling-to-strided-concat`: Convert upsampling op to strided concat op
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces `Upsampling` operations with `Concat` operations with strides and a zero filled const.
### `-convert-weights-to-u8`: Shift data from a signed range to an unsigned one
The pass is a part of `LowPrecision` pipeline.

Pass detects quantized convolution and shifts weights data from a signed range to an unsigned one
### `-decompose-lstm-cell`: Replace LSTMCell operation with a subgraph of smaller operations
Decomposes `LSTMCell` operation into smaller `DPU` friendly operations, followed by a single `LSTMGates` operation which computes activation functions.
### `-dequantize-const`: Dequantize constant tensors
The pass is a part of `LowPrecision` pipeline.

It performs constant folding for `Constant -> quant.dcast` case.
The pass is used as a fallback to FP16 computations for the cases, where quantized types where not used by layers.
### `-expand-activation-width`: Align input tensors shape of DPU operation with hardware requirements
This pass processes operations, which can be compiled as DPU tasks and
expands output width to the next number divisible by 16 when they don't
meet hardware requirements.
Applicable only for operations with NHWC input and NCHW output.
For instance, consider convolution with 16x20x23 input, 16x18x21 output and 3x3 kernel.
21 is not divisible by 16, so the output width must be expanded to 32: 16x18x32.
In order to comply to the operation traits, input width must be expanded to 16x20x34.

Supported operations:
* IE.Convolution
* IE.GroupConvolution
* IE.MaxPool
* IE.AvgPool
* IE.Add
### `-fold-relu-before-fq`: Delete ReLUOp if next Op is FakeQuantize with input_low > 0
The pass is a part of `LowPrecision` pipeline.

It deletes the ReLUOp from `ReLU -> FakeQuantize` if the FakeQuantizeOp has input_low > 0.
### `-fuse-convert-with-quantize`: Fuse Convert with Quantize into QuantCast operation
Pass detects pattern Convert(i8/ui8 -> FP16) -> Quantize(FP16 -> !quant.uniform<...>)
and fuses it into single QuantCast(i8/ui8 -> !quant.uniform<...>) operation.
### `-fuse-mem-permute`: Fuses IE.MemPermute to previous NCE task as ODU permutation
Converts these subgraphs:
```
    Input [NHWC] -> IE.Convolution [NHWC] -> IE.MemPermute [NCHW]
    Input [NHWC] -> IE.GroupConvolution [NHWC] -> IE.MemPermute [NCHW]
    Input [NHWC] -> IE.MaxPool [NHWC] -> IE.MemPermute [NCHW]
    Input [NHWC] -> IE.AvgPool [NHWC] -> IE.MemPermute [NCHW]
    Input [NHWC] -> IE.Add [NHWC] -> IE.MemPermute [NCHW]
```
Into the following subgraphs respectively:
```
    Input [NHWC] -> IE.Convolution [NCHW]
    Input [NHWC] -> IE.GroupConvolution [NCHW]
    Input [NHWC] -> IE.MaxPool [NCHW]
    Input [NHWC] -> IE.AvgPool [NCHW]
    Input [NHWC] -> IE.Add [NCHW]
```
### `-fuse-pad-ops`: Fuse PadOp with CONSTANT model
The pass is a part of `AdjustForVPU` pipeline.

PadOp with CONSTANT model, pad value is 0 and the padding is needed in H and W dimensions only.
Merge [Pad] -> [Conv] into [Conv].
Merge [Pad] -> [GroupConv] into [GroupConv].
Merge [Pad] -> [MaxPool] into [MaxPool].
### `-fuse-permute-quantize`: Converts Quantize-MemPermute combination in 1 common operation
Converts Quantize-MemPermute combination in 1 common operation.

#### Options
```
-dpu-only : [Optional] Set to true when target platform does not have software PermuteQuantize layer
```
### `-fuse-permute-quantize-expand`: Converts Quantize-MemPermute-Expand combination in 1 common operation
Converts Quantize-MemPermute-Expand combination in 1 common operation.
### `-fuse-post-ops`: Fuse activation functions with tasks that support post-processing
The pass is a part of `AdjustForVPU` pipeline.

Fuse activation functions (e.g. ReLU, leaky ReLU) with tasks that support post-processing
depending on the compilation mode
### `-fuse-quantized-ops`: Update quantize/dequantize ops
The pass is a part of `LowPrecision` pipeline.

Pass detects pattern quant.dcast -> op -> quant.qcast and converts it into single quantized Op

#### Options
```
-se-ops-enabled : Flag to identify whether operations that can be executed using the Storage Element hardware feature are enabled
```
### `-fuse-reorders`: Fuses reorder to previous NCE task as ODU permutation
Converts these subgraphs:
```
    Input [NHWC] -> IE.Convolution [NHWC] -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.GroupConvolution [NHWC] -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.MaxPool [NHWC] -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.AvgPool [NHWC] -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.Add [NHWC] -> IE.Reorder [NCHW]
```
Into the following subgraphs respectively:
```
    Input [NHWC] -> IE.Convolution [NCHW]
    Input [NHWC] -> IE.GroupConvolution [NCHW]
    Input [NHWC] -> IE.MaxPool [NCHW]
    Input [NHWC] -> IE.AvgPool [NCHW]
    Input [NHWC] -> IE.Add [NCHW]
```
### `-handle-asymmetric-strides`: Handle operations with asymmetric strides
The pass is a part of `AdjustForVPU` pipeline.

This pass splits operations so that they are able to be infered with symmetric strides
    on dpu because of hardware limitation.
### `-handle-exclude-pad-for-avg-pool`: Handle exclude-pad attribute for AvgPool operations
This pass introduces exclude pad atribute handling for AvgPool operations, that have pad = stride = 1,
by splitting operation in multiple AvgPool operations in order to handle this particular case.
### `-handle-large-kernels`: Handle large kernels ops
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces Pooling layers or Convolution layers that have kernels bigger than supported by hardware (11x11),
with equivalent Pooling (approx equiv in case of prime kernel i.e. 13x13) or Convolution layers.
### `-handle-large-pads`: Handle operations with large pads
This pass handle operations with pad larger than supported on hardware.
It will move the pad from the layer parameter to it's input. for example,
if one conv's top pad is 5, but what HW support is 2, we will set conv's pad
to 2, and concat  the input with 3 line zero constant.
### `-handle-large-strides`: Handle operations with large strides
This pass splits operations with strides larger than supported on hardware.
### `-layer-reorder-concat-pass`: Inserts Reorder operation between Transpose and Concat
The pass is a part of `HardwareMode` pipeline.

It inserts `Reorder` operation between layers `Transpose`, `AffineReshape` and `Concat` operation when possible.
This transormation reduces the number of `MemPermute` operations in resulting graph.
### `-legalize-dilated-conv`: Handle dilated convolutions
The pass is a part of `buildHardwareModePipeline` pipeline.

This pass expands filter of dilated convolution so that they are able to be infered
    on dpu because of hardware limitation.
### `-legalize-nd-mem-permute`: Legalize MemPermute operation with input rank > 4
This pass tries to legalize MemPermute operations by merging dims that are adjacent before and after the permutation.
Applied only for VPUX.37XX because SW Kernel Tiling is limited to 4D.
### `-log-op-optimizations`: Identifies operations that can be optimized into the pass' logs
Iterates over all operations in the IR and identifies those that can be optimized.
At the moment, only operations that can be optimized using the Storage Element Pointer
feature are identified.

The identified operations are mentioned in the logs of the pass. In case the
operations should already be optimized later in compilation, this will also
be mentioned in the logs.
### `-matmul-inputs-to-2d`: Convert MatMul inputs to 2d
This pass converts `MatMul` inputs to 2d.

For example, `MatMul` input with 4x1x64 geometry will be split to four inputs with 1x64 dimensions.
Resulting inputs with filters go to `MatMul` operations and the outputs are concatenated.
### `-merge-fake-quant`: Merge back to FakeQuantize
The pass is a part of `LowPrecision` pipeline.

It merges pair `quant.qcast -> quant.dcast` into single `IE.FakeQuantize`.
The pass is used as a fallback to FP16 computations for the cases, where quantized types where not used by layers.
### `-move-permute-post-eltwise`: Move the input Permute ops post Eltwise to reduce the number of Permute ops
The layout does not matter for eltwise ops as long as the input and output layouts are the same.
move the permute ops from the inputs of the eltwise to the output to reduce the number of permute ops.
### `-normalizeL2-fusion`: Convert a subgraph to normalizeL2
Convert this subgraph

    |
  /   \
 |     |
 |  ReduceL2
 |     |
 |   Clamp
  \   /
  Divide
    |
to a single normalizeL2Op
### `-optimize-concat-slice`: Bypass concat if slice is the subtensor of one of concat inputs
For the pattern ConcatOp->SliceOp, if SliceOp input is the subtensor of one of ConcatOp input,
Bypass ConcatOp and ConcatOp would be removed if it has only one user.
### `-optimize-reorders`: Optimize extra Reorder operations
The pass is a part of `IECommon` pipeline.

This pass tries to optimize out Reorder operations for common cases
by propagating them from inputs to outputs and merging into layers.

#### Options
```
-se-ops-enabled : Flag to identify whether operations that can be executed using the Storage Element hardware feature are enabled
```
### `-optimize-unaligned-qdq-seq`: Swaps AffineReshape->FakeQuantize sequence if channels become unaligned after AffineReshape
Pass swaps order of AffineReshape->FakeQuantize sequence if channels become unaligned after AffineReshape
Otherwise additionals ops are introduce in order to align channels which impacts performance.
### `-per-axis-fq-concat`: Supports Concat operation with per-axis FQ inputs
The pass is a part of `HardwareMode` pipeline.

It creates `FakeQuantize` operation, which combines per-channel quantization from `Concat` inputs,
and places it after the `Concat` operation. For example:
The following `Concat`:
```
    FQ 1x256x128x128 -> Concat <- FQ 1x48x128x128
                          |
                        GroupConv 1x304x128x128
```
will be transformed into:
```
    FQ 1x256x128x128 -> Concat <- FQ 1x48x128x128
                          |
                         FQ 1x304x128x128
                          |
                        GroupConv 1x304x128x128
```
### `-propagate-affine-reshape`: Moves AffineReshape operation down
Supported cases:
* Move through Transpose
* Move through Expand
* Move through Concat // TODO: #-58713
    Before:
        AffineReshape ->
        AffineReshape -> Concat
        AffineReshape ->
    After:
        Concat -> AffineReshape
* Move through Softmax
### `-propagate-expand`: Propagate Expand operation in order to fuse it with other layers
Propagate Expand through Eltwise Add in case layers before might be fused with Expand
in following cases:
1. PermuteQuntize might be fused with Expand in FusePermuteQuantizeExpand pass
2. DepthToSpace is used with padded channels' descriptor
3. SpaceToDepth might be executed with expanded input on dpu with following convolution with padded filter
### `-propagate-fq-through-concat`: Propagate FakeQuantize operation through Concat
`ConvertPadToConcat` adds a `Concat` operation which does not propagate `FakeQuantize` operation.

1. Check if such `Concat` operation has one and only one quantized input
2. Fetch quantization parameters
3. Apply them to every single `Concat` input and output
### `-propagate-mem-permute-before-op`: Propagate IE.MemPermute through concrete op
Propagates permute through concrete op.
1. Converts this subgraph
```
    IE.AffineReshape -> IE.MemPermute
```
into
```
    [IE.PermuteCast] -> IE.MemPermute -> IE.Reshape -> IE.PermuteCast
```
2. Converts this subgraph
```
    IE.MVN -> IE.MemPermute
```
into
```
    IE.MemPermute -> IE.MVN-> IE.PermuteCast
```

Canonicalization may fuse IE.MemPermute with another one.
### `-propagate-mem-permute-through-add`: Propagates IE.MemPermute through IE.Add when both inputs of IE.Add have IE.MemPermute
Propagates last permute in the chain. Converts this subgraph
```
    IE.MemPermute -> IE.ShapeCast \
                                   IE.Add -> IE.ShapeCast -> IE.MemPermute
    IE.MemPermute -> IE.ShapeCast /
```
into
```
    IE.MemPermute -> IE.MemPermute -> IE.ShapeCast \
                                                    IE.Add -> IE.ShapeCast
    IE.MemPermute -> IE.MemPermute -> IE.ShapeCast /
```
Canonicalization may simplify this when IE.MemPermute operations cancel one another
### `-propagate-mem-permute-through-softmax`: Propagate IE.MemPermute through IE.SoftMaxOp
Propagates permute through SoftMaxOp. Converts this subgraph
```
    IE.MemPermute -> IE.ShapeCast \
                                   IE.Add -> IE.ShapeCast -> IE.MemPermute
    IE.MemPermute -> IE.ShapeCast /
```
into
```
    IE.MemPermute -> IE.MemPermute -> IE.ShapeCast \
                                                    IE.Add -> IE.ShapeCast
    IE.MemPermute -> IE.MemPermute -> IE.ShapeCast /
```
Canonicalization may simplify this when IE.MemPermute operations cancel one another
### `-propagate-mem-permute-through-softmax`: Propagate IE.MemPermute through IE.SoftMaxOp
Propagates permute through SoftMaxOp. Converts this subgraph
```
    IE.SoftMaxOp -> IE.MemPermute
```
into
```
    IE.MemPermute -> IE.SoftMaxOp
```
Canonicalization may fuse IE.MemPermute with another one.
### `-propagate-op-through-batch-concat`: Propagate SW ops after batch unrolled matmul to enable vertical fusion
Move ops after concat to place after each batch unrolled matmul.
Currently only softmax is enabled.

### `-propagate-quantize-dequantize`: Propagate Quantize/Dequantize through agnostic operations
The pass is a part of LowPrecision pipeline.

Quantize/Dequantize are propagated through operations
### `-propagate-reorder-to-nce`: Propagate reorder back to NCE task through act shave layers
Converts these subgraphs:
```
    Input [NHWC] -> IE.Convolution [NHWC] -> Act shave layer -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.GroupConvolution [NHWC] -> Act shave layer -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.MaxPool [NHWC] -> Act shave layer -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.AvgPool [NHWC] -> Act shave layer -> IE.Reorder [NCHW]
    Input [NHWC] -> IE.Add [NHWC] -> Act shave layer -> IE.Reorder [NCHW]
```
Into the following subgraphs respectively:
```
    Input [NHWC] -> IE.Convolution [NHWC] -> IE.Reorder [NCHW] -> Act shave layer [NCHW]
    Input [NHWC] -> IE.GroupConvolution [NHWC] -> IE.Reorder [NCHW] -> Act shave layer [NCHW]
    Input [NHWC] -> IE.MaxPool [NHWC] -> IE.Reorder [NCHW] -> Act shave layer [NCHW]
    Input [NHWC] -> IE.AvgPool [NHWC] -> IE.Reorder [NCHW] -> Act shave layer [NCHW]
    Input [NHWC] -> IE.Add [NHWC] -> IE.Reorder [NCHW] -> Act shave layer [NCHW]
```
### `-propagate-transpose`: Moves Transpose operation down
Supported cases:
* Move through Softmax
### `-remove-identity-pools`: Remove identiy pools
The pass removes the identity ops
Because we have passes which introduce such identity pools, we can't have this
as a folder/canonicalizer
### `-remove-quantdequant-seq`: Removes quantize->dequantize ops sequence
The optional pass in the `LowPrecision` pipeline.

Pass detects pattern quantize -> dequantize and removes it
### `-remove-view-like-ops-chain`: Remove View-Like ops chain if first view-like op's input is same as the last view like op's output
Remove View-Like ops chain if first view-like op's input is same as the last view like op's output.

Converts subgraph like:
```
LayerOp -> PermuteCastOp1 -> AffineReshape -> PermuteCastOp2 -> LayerOp
```
Into
```
LayerOp -> LayerOp
```
if PermuteCastOp1's input == PermuteCastOp2's output.
### `-resolve-scatter-update-by-transpose`: Resovle ScatterUpdate operation by Transpose Operation
The pass is a part of `AdjustForVPU` pipeline.
Only axis == 0 is supported in SWkernel.
The pass replaces ScatterUpdate(axis!=0) with `IE::Transpose-IE::ScatterUpdate(axis=0)-IE::Transpose` pipeline.

### `-resolve-strided-slice`: Decouple strided slice to slice + reshape
The pass is a part of `AdjustForVPU` pipeline.
It replaces IE::StridedSlice with non zero masks to a simpler IE::StridedSlice with zero masks + IE::Reshape
It replaces IE::StridedSlice with dense<1> strides strides with a simple IE::Slice operation
### `-split-conv-with-multiple-fq`: Splits Convolution for multiple FakeQuantize
The pass is a part of `HardwareMode` pipeline.

It splits `Convolution` operation with multiple consumers with `FakeQuantize` operations,
into multiple `Convolution` operations, one for each consumer. This transformation is needed to be
able to quantize convolution and fuse bias and post-processing operations.
### `-split-fake-quant`: Splits FakeQuantize
The pass is a part of `LowPrecision` pipeline.

It splits `FakeQuantize` operations to `quant.qcast -> quant.dcast` pair.
### `-swap-convert-with-transpose-reshape`: Swaps Transpose operation with Convert
The pass is a part of `HardwareMode` pipeline.

It swaps `Transpose` and 'Reshape' operations with Convert operation when possible.
This transormation reduces the number of `MemPermute` operations in resulting graph.
### `-swap-fake-quant-with-reshape-and-strided-slice`: Swap FakeQuantize with Reshape and StridedSlice when required to void redundant expand and permute ops
The pass is a part of `LowPrecision` pipeline.

It matches pattern non-channel-aligned op -> optional Reshapes -> FQ -> Reshapes / StridedSlice -> channel-aligned op
Move the FQ right before the channel-aligned op to avoid redundant expand and permute ops.
### `-swap-maxpool-with-act`: Swaps the MaxPool and activation
This pass is needed for VPUX37XX only since HW MaxPool does not support post-op operations.
Operations are swapped only if there is an operation before MaxPool that supports post-ops.
### `-swap-mvn-with-transpose`: Swaps MVN operation with parent Transpose
The pass is a part of `HardwareMode` pipeline.

It swaps `MVN` with Transpose operation when possible.
This transormation reduces the number of `MemPermute` operations in resulting graph.
### `-swap-operations`: Swap operations implemented ElemTypeInfoOpInterface interface with bias and activation
In order to fuse the bias and activation functions into a main operation,
intermediate operations will be moved after the fusable operation. For example:
  `Conv -> Reshape -> bias` will be converted to `Conv -> bias -> Reshape`
  `Conv -> Transpose -> ReLU` will be converted to `Conv -> ReLU -> Transpose
Only operations that implement `IE_ElemTypeInfoOpInterface` are moved.
Currently, operations are moved only through bias (Add), ReLU, Sigmoid, Tanh and Clamp.
### `-swap-pad-layer`: Swap pattern Pad -> Transpose to Transpose -> Pad
In order to fuse Pad layer to Convolution swap Pad with operations between it and Convolution.
For now only case Pad -> Transpose is supported

### `-swap-quant-cast-and-clamp`: Swap QuantizeCast and Clamp operations
After AlignScales pass we have an additional Clamp layers in IR.
Therefore, we may get such subgraph:
ARG -> FQ -> Clamp -> Concat

Then after SplitFakeQuant and PropagateQuantizeDequantize we have:
ARG -> Q -> Clamp -> D -> Concat

Then after ConvertQuantizeOpsToNceOps we have:
ARG -> Add -> QuantizeCast -> Clamp -> QuantizeCast -> Add -> Concat

In order to fuse Clamp into eltwise Add we need to move QuantizeCast after Clamp.
### `-swap-transpose-concat`: Swap Transpose and Concat operations
Pass converts pattern from
Transpose ->
Transpose -> Concat
Transpose ->

to
Concat -> Transpose
### `-swap-transpose-with-fq`: Swaps Transpose operation with FakeQuantize
The pass is a part of `HardwareMode` pipeline.

It swaps `Transpose` operation with per-tensor `FakeQuantize` operation when possible.
This transormation reduces the number of `MemPermute` operations in resulting graph.
### `-transpose-to-permute-cast`: Converts Transpose operation to PermuteCast with Reorder
It is possible to replace a `Transpose` operation with a combination of `PermuteCast` and `Reorder`.
To compute the permutation cast, which is required for the source tensor, one must inverse the
affine map from the original `Transpose` operation. For example, consider the following transposition:
`1x16x32x64 -> 1x64x16x32`, its affine map is: `(d0, d1, d2, d3) -> (d0, d3, d1, d2)`.
The inverse will be:
```
    d0, d3, d1, d2   ->  d0, d1, d2, d3
    aN, aC, aH, aW   ->  aN, aH, aW, aC
```
Which gives permutation cast into NHWC.
In order to maintain the layout in data flow, `Reorder` must always rearrange `PermuteCast` result into the
order of original `Transpose` operation.
### `-uniquify-branches`: Eliminates redundant operations from multiple branches
Convert this subgraph:
         -> Slice -> Layer -> Consumer
Producer -> Slice -> Layer -> Consumer
         -> Slice -> Layer -> Consumer

into this:
                  -> Slice -> Consumer
Producer -> Layer -> Slice -> Consumer
                  -> Slice -> Consumer

in case Slice and Layer transform different axes.

Now at the place of "Layer" supported Reorder and Expand operations
### `-uniquify-ops`: Remove duplicating operations with a common producer Value
The pass is a part of `AdjustForVPU` pipeline.

This pass merges operations that are identical to each other, combining consumers.
### `-upstream-slice`: Optimization by upstreaming slice operations
Optimizes scenarios of IE::StridedSlice and IE::SliceOp without neighboring operations.
Moves the slice operations upwards through the graph, reducing both compute and memory usage.
In some cases the slice operation may be safely removed from the graph, if the action of upstreaming it
    only adapts the operations constants.
### `-use-user-precision`: Use user precisions for entry point function prototype
The pass is a part of `IECommon` pipeline.

This pass updates the CNNNetwork entry point function prototype and use user-provided precisions for its operands and results.
The pass inserts Convert operations from/to topology precisions.
